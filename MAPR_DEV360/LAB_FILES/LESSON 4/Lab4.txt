/*_________________________________________________________________________*/
/************Lab 4.1.2 - Load Data into Apache Spark **********/
//Map input variables
val IncidntNum = 0
val Category = 1
val Descript = 2
val DayOfWeek = 3
val Date = 4
val Time = 5
val PdDistrict = 6
val Resolution = 7
val Address = 8
val X = 9
val Y = 10
val PdId = 11

//Load SFPD data into RDD
val sfpdRDD = sc.textFile("/user/user01/data/sfpd.csv").map(line=>line.split(","))

/*_________________________________________________________________________*/
/**************LAB 4.2.1 Explore Data Using RDD Operations******************/
//1. How do you see the first element of the inputRDD?
sfpdRDD.first()

//2.What do you use to see the first 5 elements of the RDD?
sfpdRDD.take(5)

//3. What is the total number of incidents?
val totincs = sfpdRDD.count()

//4. What is the total number of distinct resolutionss?
val totres = sfpdRDD.map(inc=>inc(Resolution)).distinct.count

//4. List all the Districts.
val dists = sfpdRDD.map(inc=>inc(PdDistrict)).distinct
dists.collect

/*_________________________________________________________________________*/
/*************LAB 4.3.1 - Create PAIR RDDs & apply pairRDD operations**************/
//1. Which five districts have the highest incidents?
val top5Dists = sfpdRDD.map(incident=>(incident(PdDistrict),1)).reduceByKey((x,y)=>x+y).map(x=>(x._2,x._1)).sortByKey(false).take(5)

//2. Which five addresses have the highest number of incidents?
val top5Adds = sfpdRDD.map(incident=>(incident(Address),1)).reduceByKey((x,y)=>x+y).map(x=>(x._2,x._1)).sortByKey(false).take(5)

//3. What are the top three catefories of incidents?
val top3Cat = sfpdRDD.map(incident=>(incident(Category),1)).reduceByKey((x,y)=>x+y).map(x=>(x._2,x._1)).sortByKey(false).take(3)

//4. What is the count of incidents by district?
val num_inc_dist = sfpdRDD.map(incident=>(incident(PdDistrict),1)).countByKey()

/*_________________________________________________________________________*/
/**************************Lab 4.3.2. Join PairRDD**************************/
//5. Load each dataset into separate pairRDDs with “address” being the key? 
val catAdd = sc.textFile("/user/user01/data/J_AddCat.csv").map(x=>x.split(",")).map(x=>(x(1),x(0)))
val distAdd = sc.textFile("/user/user01/data/J_AddDist.csv").map(x=>x.split(",")).map(x=>(x(1),x(0)))


//6. List the incident category and district for for those addresses that have both category and district information. Verify that the size estimated earlier is correct.
val catJdist = catAdd.join(distAdd)
catJdist.collect
catJdist.count
catJdist.take(10)

//7.	List the incident category and district for all addresses irrespective of whether each address has category and district information. Verify that the size estimated earlier is correct.
val catJdist1 = catAdd.leftOuterJoin(distAdd)
catJdist1.collect
catJdist.count

//8.	List the incident district and category for all addresses irrespective of whether each address has category and district information. Verify that the size estimated earlier is correct.
val catJdist2 = catAdd.rightOuterJoin(distAdd)
catJdist2.collect
catJdist2.count
/*_________________________________________________________________________*/
/**************************Lab 4.4.1. Join PairRDD**************************/
//1. How many partitions are there in the sfpdRDD?
sfpdRDD.partitions.size

//2. How do you find the type of partitioner?
sfpdRDD.partitioner

//3. Create a pair RDD - INcidents by district
val incByDists = sfpdRDD.map(incident=>(incident(PdDistrict),1)).reduceByKey((x,y)=>x+y)
//How many partitions does this have?
incByDists.partitions.size
//What is the type of partitioner?
incByDists.partitioner

//4. Now add a map transformation
val inc_map = incByDists.map(x=>(x._2,x._1))
//Is there a change in the size?
inc_map.partitions.size
//What about the type of partitioner?
inc_map.partitioner

//5.Add sortByKey()
val inc_sort=incByDists.incByDists.map(x=>(x._2,x._1)).sortByKey(false)
//type of partitioner
inc_sort.partitioner

//6. Add groupByKey
val inc_group = sfpdRDD.map(incident=>(incident(PdDistrict),1)).groupByKey()
//type of partitioner

//7. specify partition size in the transformation
val incByDists = sfpdRDD.map(incident=>(incident(PdDistrict),1)).reduceByKey((x,y)=>x+y,10)
//number of partitions
incByDists.partitions.size

//8. Create 2 pairRDD
val catAdd = sc.textFile("/user/user01/data/J_AddCat.csv").map(x=>x.split(",")).map(x=>(x(1),x(0)))
val distAdd = sc.textFile("/user/user01/data/J_AddDist.csv").map(x=>x.split(",")).map(x=>(x(1),x(0)))

//9. join and specify partitions- then check the number of partitions and the partitioner
val catJdist=catAdd.join(distAdd,8)
catJdist.partitions.size
catjDist.partitioner

